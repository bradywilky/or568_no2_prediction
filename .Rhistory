source("~/repositories/or568_no2_prediction/training_data.R", echo=TRUE)
View(t)
View(nonzero_train)
nrow(t[t$val == 0,])
t <- data.frame(
initial_mod$residuals
, initial_mod$fitted.values
, train$value
, train$date
, train$location
)
colnames(t) <- c('res', 'fit.val', 'val', 'date', 'loc')
nrow(t[abs(t$res) > 100,])
nrow(t[abs(t$res) > 500,])
nrow(t[abs(t$res) > 300,])
nrow(t[abs(t$res) > 200,])
nrow(t[abs(t$res) > 150,])
t[abs(t$res) > 150,]
t[abs(t$res) > 100,]
t[abs(t$res) > 150,]
res_100 <- t[abs(t$res) > 100,]
res_150 <- t[abs(t$res) > 150,]
aggregate(data.frame(count=res_150$loc), list(value=res_150$loc), length)
colnames(t) <- c('res', 'fit.val', 'val', 'date', 'loc')
res_150 <- t[abs(t$res) > 150,]
res_200 <- t[abs(t$res) > 200,]
nrow(res_150)
nrow(res_200)
aggregate(data.frame(count=res_150$loc), list(value=res_150$loc), length)
aggregate(data.frame(count=res_150$loc), list(value=res_200$loc), length)
aggregate(data.frame(count=res_200$loc), list(value=res_200$loc), length)
aggregate(data.frame(count=res_100$loc), list(value=res_100$loc), length)
library(magrittr) # used only to import the operator %>%
library(stringr) # for various character vector operations
# required data:
#   train_labels.csv
#   grid_metadata.csv
train_labels <- read.csv("~/repositories/or568_no2_prediction/data/train_labels.csv")
grid_meta <- read.csv("~/repositories/or568_no2_prediction/data/grid_metadata.csv")
# Packages to use
library(ggplot2) # various plotting
library(dplyr) # data manipulation, also includes the operator %>%
library(magrittr) # explicit import to work around error with dplyr
library(stringr) # for various character vector operations
# required data for this script:
#   train_labels.csv (download)
#   grid_metadata.csv (download)
#   sat_vcd_data.csv (run read_satellite_data.R)
train_labels <- read.csv("~/repositories/or568_no2_prediction/data/train_labels.csv")
grid_meta <- read.csv("~/repositories/or568_no2_prediction/data/grid_metadata.csv")
###############################
# Transforming grid coordinates
###############################
# The goal here is to get a latitude and longitude value for each cell, so that
# location data can be joined to each cell.
# each cell's grid coordinates are stored in the wkt column, which specifies coordinates
# for a polygon to plot on a map. Extracting latitude and longitude from this column
# and storing it in our own columns.
grid_meta$temp <- grid_meta$wkt %>% str_remove(
"POLYGON \\(\\(") %>% str_remove(
"\\)\\)") %>% str_remove_all(
",") %>% strsplit(split=" ")
# indices where latitude and longitude reside
i_lon <- c(1,3,5,7)
i_lat <- c(2,4,6,8)
# setting up empty vectors for our later columns. Each cell is defined by four points:
# min latitude, max latitude, min longitude, and max longitude.
max_lat <- vector(mode="numeric")
max_lon <- vector(mode="numeric")
min_lat <- vector(mode="numeric")
min_lon <- vector(mode="numeric")
# iterating through each grid's polygon coordinates and assigning each coordinate
# to the appropriate vector
for(i in grid_meta$temp){
max_lat <- c(max_lat, max(as.numeric(i[i_lat])))
min_lat <- c(min_lat, min(as.numeric(i[i_lat])))
max_lon <- c(max_lon, max(as.numeric(i[i_lon])))
min_lon <- c(min_lon, min(as.numeric(i[i_lon])))
}
# creating new columns from vectors created above
grid_meta$max_lat <- max_lat
grid_meta$max_lon <- max_lon
grid_meta$min_lat <- min_lat
grid_meta$min_lon <- min_lon
# these columns should be the geometric center of the grid, representing its
# unofficial coordinate point. Can be used to join data with latitude and
# longitude.
grid_meta$mid_lat <- rowMeans(grid_meta[,c("max_lat", "min_lat")], na.rm=TRUE)
grid_meta$mid_lon <- rowMeans(grid_meta[,c("max_lon", "min_lon")], na.rm=TRUE)
# creating and saving grid_meta transformed
grid_meta_trns <- grid_meta %>% subset(select = -c(temp, wkt))
path <- ("~/repositories/or568_no2_prediction/data/grid_meta_trns.csv")
grid_meta_trns %>% write.csv(path, row.names=FALSE, quote=FALSE)
# joining transformed grid_meta with the training labels
train_data <- merge(train_labels, grid_meta_trns, by = "grid_id")
#########################
## Engineering Predictors
#########################
# creating month predictor
train_data$month <- substr(train_data$datetime, 6, 7)
## creating predictors with LA weather data
la_weather <- read.csv("~/repositories/or568_no2_prediction/data/LAweather_data.csv")
unique(la_weather$location)
unique(train_data$location)
train_data$location[train_data$location == 'Los Angeles (SoCAB)'] <- "Los Angeles"
la_weather$location <- "Los Angeles"
## creating predictors from NO2 vertical column density daily readings
# reading NO2 vertical column density satellite data
path <- ("~/repositories/or568_no2_prediction/data/sat_vcd_data.csv")
no2_vcd <- read.csv(path)
# transforming date column to join NO2 vertical column density on
train_data$date <- substr(train_data$datetime,1,10) %>% str_replace_all("-", " ")
# joining NO2 VCD dataframe to the labeled data
train_data <- merge(train_data, no2_vcd, by = c("location", "date"))
# saving data
path <- ("C:/Users/15714/Documents/repositories/or568_no2_prediction/data/train_data.csv")
write.csv(train_data, path)
## EDA
# Explore the data
unique(train_data$location)
summary(train_data$value)
train_data %>% group_by(location) %>% summarise(count_entries = n())
# Plots
ggplot(train_data, aes(value))+ geom_density(fill="blue") + ggtitle("NO2 Value Distribution")
ggplot(train_data, aes(log(value))) + geom_density(fill="blue") + ggtitle("NO2 Log Value Distribution")
ggplot(train_data, aes(sqrt(value))) + geom_density(fill="blue") + ggtitle("NO2 Square Root Value Distribution")
gx = train_data %>%
group_by(location) %>%
summarise(value = mean(value))
gx
gx %>% ggplot(aes(x = location, y = value, color = location)) +
geom_bar(stat = "identity", alpha = 0.3) +
theme_classic() +
labs(y = "NO2 Value",
x = "Location")
gx = train_data %>%
group_by(month) %>%
summarise(value = mean(value))
gx
gx %>% ggplot(aes(x = month, y = value, color = month)) +
geom_bar(stat = "identity", alpha = 0.3) +
theme_classic() +
labs(y = "NO2 Value",
x = "Month")
## EDA
path <- path <- ("C:/Users/15714/Documents/repositories/or568_no2_prediction/data/train_data.csv")
train_data <- read.csv(path)
source("~/repositories/or568_no2_prediction/eda.R", echo=TRUE)
library(ggplot2) # various plotting
source("~/repositories/or568_no2_prediction/training_data.R", echo=TRUE)
## Anti-joining on outliers to see how results are with new dataframe
boxplot.stats(train_data_nolab$value)$out
# Looking at outliers with top percentiles
quantile(train_data_nolab$value, 0.975)
# Looking at outliers with top percentiles
quantile(train_data_nolab$value, 0.995)
# Looking at outliers with top percentiles
cutoff <- quantile(train_data_nolab$value, 0.995)
train_data_nolab[abs(train_data_nolab$value) < cutoff,]
train_data_nolab[abs(train_data_nolab$value) > cutoff,] %>% nrow()
# Looking at outliers with top percentiles
cutoff <- quantile(train_data_nolab$value, 0.9995)
train_data_nolab[abs(train_data_nolab$value) > cutoff,] %>% nrow()
top_percentile <- 0.9995
cutoff <- quantile(train_data_nolab$value, top_percentile)
train_data_nolab[abs(train_data_nolab$value) > cutoff,] %>% nrow()
train_data_nolab_outliers <- train_data_nolab[abs(train_data_nolab$value) > cutoff,]
View(train_data_nolab_outliers)
# tdnno = train_data_nolab_no_outliers
tdnno <- anti_join(
train_data_nolab, train_data_nolab_outliers, by=c("location", "state"))
# splitting the new data into train and test
row.number <- sample(1:nrow(tdnno), 0.8*nrow(tdnno))
train = tdnno[row.number,]
test = tdnno[-row.number,]
# creating model with all predictors
initial_mod <- lm(value~., train)
summary(initial_mod)
tdnno <- anti_join(
train_data_nolab, train_data_nolab_outliers, by=c("location", "date"))
row.number <- sample(1:nrow(tdnno), 0.8*nrow(tdnno))
train = tdnno[row.number,]
test = tdnno[-row.number,]
# creating model with all predictors
initial_mod <- lm(value~., train)
summary(initial_mod)
# Looking at outliers with top percentiles
top_percentile <- 0.995
cutoff <- quantile(train_data_nolab$value, top_percentile)
train_data_nolab[abs(train_data_nolab$value) > cutoff,] %>% nrow()
train_data_nolab_outliers <- train_data_nolab[abs(train_data_nolab$value) > cutoff,]
# tdnno = train_data_nolab_no_outliers
tdnno <- anti_join(
train_data_nolab, train_data_nolab_outliers, by=c("location", "date"))
# splitting the new data into train and test
row.number <- sample(1:nrow(tdnno), 0.8*nrow(tdnno))
train = tdnno[row.number,]
test = tdnno[-row.number,]
# creating model with all predictors
initial_mod <- lm(value~., train)
summary(initial_mod)
# Creating temporary column to group by. Need to aggregate data by week and location
# and get mean values.
tdnno$week_month_loc <- substr(tdnno$date,1,8)
View(tdnno)
# Creating temporary column to group by. Need to aggregate data by week and location
# and get mean values.
tdnno$week_month_loc <- paste(substr(tdnno$date,1,7), tdnno$location)
# SQL mean group by month week location
aggregate(tdnno$value, list(value=tdnno$week_month_cell), mean)
list(value=tdnno$week_month_cell)
# SQL mean group by month week location
aggregate(tdnno$value, list(tdnno$week_month_cell), mean)
list(tdnno$week_month_cell)
tdnno$week_month_cell <- paste(
substr(tdnno$date,1,7)
, tdnno$mid_lat
, tdnno$mid_lon
)
# SQL mean group by month week location
aggregate(tdnno$value, list(tdnno$week_month_cell), mean)
# SQL mean group by month week location
grouped_df <- aggregate(tdnno$value, list(tdnno$week_month_cell), mean)
View(grouped_df)
library(sqldf) # prefer to use SQL aggregate syntax for grouping data
install.packages('sqldf')
library(sqldf) # prefer to use SQL aggregate syntax for grouping data
sqldf('select * from tdnno')
?sqldf
sqldf('SELECT * FROM tdnno')
sqldf('SELECT mid_lon FROM tdnno')
sqldf('SELECT mid_lat, mid_lon FROM tdnno')
sqldf('SELECT mid_lat, mid_lon, location FROM tdnno')
query <- '
SELECT
avg(vcd_no2) AS vcd_no2
, avg(value) AS value
, mid_lat, mid_lon, location, month, week_month_cell
FROM
tdnno
GROUP BY mid_lat, mid_lon, location, month, week_month_cell
'
sqldf(query)
?aggregate.data.frame
source("~/repositories/or568_no2_prediction/training_data.R", echo=TRUE)
aggregate(tdnno[, c("vcd_no2", "value")], list(tdnno$value), mean)
aggregate(tdnno[, c("vcd_no2", "value")], list(tdnno$week_month_cell), mean)
grouped_df <- aggregate(tdnno[, c("vcd_no2", "value")], list(tdnno$week_month_cell), mean)
View(grouped_df)
View(grouped_df)
grouped_df$mid_lat <- strsplit(grouped_df$Group.1, split = " ")[1]
grouped_df$mid_lat <- strsplit(grouped_df$Group.1, split = " ")[[1]]
View(tdnno)
add_cols <- tdnno[, c("location", "month", "mid_lat_sc", "mid_lon_sc", "week_month_cell")]
df <- merge(grouped_df, add_cols, by = "week_month_cell")
add_cols
add_cols <- tdnno[, c("location", "month", "mid_lat_sc", "mid_lon_sc", "week_month_cell")] %>% distinct()
df <- merge(grouped_df, add_cols, by = "week_month_cell")
source("~/repositories/or568_no2_prediction/training_data.R", echo=TRUE)
add_cols <- tdnno[, c("location", "month", "mid_lat_sc", "mid_lon_sc", "week_month_cell")] %>% distinct()
df <- merge(grouped_df, add_cols, by = "week_month_cell")
grouped_df[, "week_month_cell"]
View(grouped_df)
grouped_df <- aggregate(tdnno[, c("vcd_no2", "value")], list(tdnno$week_month_cell), mean)
names(grouped_df)[names(grouped_df) == "Group.1"] <- "week_month_cell"
add_cols <- tdnno[, c("location", "month", "mid_lat_sc", "mid_lon_sc", "week_month_cell")] %>% distinct()
df <- merge(grouped_df, add_cols, by = "week_month_cell")
grouped_df[, "week_month_cell"]
df <- grouped_df[, "week_month_cell"]
df <- merge(grouped_df, add_cols, by = "week_month_cell")
View(df)
df <- merge(grouped_df, add_cols, by = "week_month_cell") %>% subset(select=-week_month_cell)
new_train <- merge(grouped_df, add_cols, by = "week_month_cell") %>% subset(select=-week_month_cell)
View(new_train)
new_training_data <- merge(grouped_df, add_cols, by = "week_month_cell") %>% subset(select=-week_month_cell)
# saving data
path <- ("C:/Users/15714/Documents/repositories/or568_no2_prediction/data/grouped_train_data.csv")
write.csv(new_training_data, path)
# splitting the new data into train and test
row.number <- sample(1:nrow(new_training_data), 0.8*nrow(new_training_data))
train = tdnno[row.number,]
test = tdnno[-row.number,]
# creating model with all predictors
initial_mod <- lm(value~., train)
summary(initial_mod)
row.number <- sample(1:nrow(new_training_data), 0.8*nrow(new_training_data))
train = new_training_data[row.number,]
test = new_training_data[-row.number,]
# creating model with all predictors
initial_mod <- lm(value~., train)
summary(initial_mod)
?nnet
x.tr <- train %>% subset(select=-value)
y.tr <- train %>% subset(select=value)
nnet <- train(x.tr, y.tr,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(x.tr) + 1) + 10 + 1,
maxit = 500)
# 12.2
library(caret)
nnet <- train(x.tr, y.tr,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(x.tr) + 1) + 10 + 1,
maxit = 500)
y.tr <- train$value
nnet <- train(x.tr, y.tr,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(x.tr) + 1) + 10 + 1,
maxit = 500)
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10), .bag = FALSE)
nnet <- train(x.tr, y.tr,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(x.tr) + 1) + 10 + 1,
maxit = 500)
x.tr <- train %>% subset(select=-value)
y.tr <- train$value
x.te <- test %>% subset(select=-value)
y.te <- test$value
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10), .bag = FALSE)
nnet <- train(x.tr, y.tr,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(x.tr) + 1) + 10 + 1,
maxit = 500)
nnet.pred <- predict(nnet, newdata = x.te)
postResample(pred = nnet.pred, obs = y.te)
lmod <- lm(value~., train)
lm.pred <- predict(lmod, newdata = x.te)
postResample(pred = lm.pred, obs = y.te)
postResample(pred = lm.pred, obs = y.te)
lasso <- train(x.tr, y.tr,
method = 'glmnet',
tuneGrid = expand.grid(alpha = 1, lambda = 1)
)
lasso <- train(x = x.tr, y = y.tr,
method = 'glmnet',
tuneGrid = expand.grid(alpha = 1, lambda = 1)
)
lasso <- train(x = x.tr, y = y.tr,
method = 'glmnet')
lasso.pred <- predict(lasso, newdata = x.te)
lasso <- train(x = x.tr, y = y.tr,
method = 'glmnet',
tuneGrid = expand.grid(alpha = 0, lambda = 1)
)
library(glmnet)
cv_model <- cv.glmnet(x.tr, y.tr, alpha = 1)
as.matrix(x.tr)
as.data.frame(x.tr)
lasso <- train(x = as.matrix(x.tr), y = y.tr,
method = 'glmnet',
tuneGrid = expand.grid(alpha = 1, lambda = 1)
)
cv_model <- cv.glmnet(as.matrix(x.tr), y.tr, alpha = 1)
is.na(x.tr)
x.tr[is.na(x.tr)]
cv_model <- cv.glmnet(as.matrix(x.tr), y.tr, alpha = 1)
cv_model <- cv.glmnet(as.data.frame(x.tr), y.tr, alpha = 1)
cv_model <- cv.glmnet(data.matrix(x.tr), y.tr, alpha = 1)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
# ridge
ridge <- cv.glmnet(data.matrix(x.tr), y.tr, alpha = 0)
ridge$lambda.min
